{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Financial Ratio Calculation for Quantile Strategy\n",
    "\n",
    "This notebook calculates three financial ratios on a daily basis for the quantile-based long-short trading strategy:\n",
    "\n",
    "1. **Debt to Market Cap** - Leverage indicator\n",
    "2. **Return on Investment (ROI)** - Profitability relative to capital employed\n",
    "3. **Price to Earnings (P/E)** - Valuation metric\n",
    "\n",
    "**Key Methodology:**\n",
    "- Fundamental data (debt, earnings, ROI) updates at filing dates and forward-fills\n",
    "- Market capitalization updates daily with stock prices\n",
    "- Ratios recalculate daily as prices change\n",
    "\n",
    "\n",
    "\n",
    "The link to the GitHub Repository with full code base to backtester and strategy code is here:\n",
    "\n",
    "https://github.com/sidsahacodes/qts/tree/main/quantile_strats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": [
    "## Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "BASE_DIR = Path(r\"C:\\Users\\15126\\Desktop\\Chicago\\Winter\\qts\\hw\\quantile_strats\\Data_export\\ZacksFundamentalsB\")\n",
    "UNIVERSE_PATH = Path(r'C:\\Users\\15126\\Desktop\\Chicago\\Winter\\qts\\hw\\quantile_strats\\investment_universe.csv')\n",
    "\n",
    "PERIOD_START = pd.to_datetime(\"2018-01-01\")\n",
    "PERIOD_END = pd.to_datetime(\"2023-06-30\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "universe",
   "metadata": {},
   "source": [
    "## Load Investment Universe\n",
    "\n",
    "Random sample of 500 tickers from the pre-filtered investment universe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "load-universe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Universe: 500 tickers\n",
      "Sample: ['ACEL', 'ACIW', 'AENZ', 'AGR', 'AGRO', 'AHCO', 'AIMC', 'ALGT', 'ALK', 'ALNT']\n"
     ]
    }
   ],
   "source": [
    "universe_df = pd.read_csv(UNIVERSE_PATH)\n",
    "\n",
    "np.random.seed(42)\n",
    "universe_df = universe_df.sample(n=500, random_state=42)\n",
    "universe_tickers = set(universe_df['ticker'])\n",
    "\n",
    "print(f\"Universe: {len(universe_tickers)} tickers\")\n",
    "print(f\"Sample: {sorted(universe_tickers)[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-data",
   "metadata": {},
   "source": [
    "## Load Fundamental and Price Data\n",
    "\n",
    "Load and filter Zacks Fundamentals B tables and QuoteMedia price data to the universe and analysis period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "load-fundamentals",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading fundamental data tables...\n",
      "MKTV: 11,000 | FC: 13,276 | FR: 13,276 | SHRS: 11,000\n"
     ]
    }
   ],
   "source": [
    "def load_zacks_table(zip_name):\n",
    "    with zipfile.ZipFile(BASE_DIR / zip_name) as z:\n",
    "        csv_files = [f for f in z.namelist() if f.endswith(\".csv\")]\n",
    "        with z.open(csv_files[0]) as f:\n",
    "            return pd.read_csv(f, low_memory=False)\n",
    "\n",
    "print(\"Loading fundamental data tables...\")\n",
    "\n",
    "mktv = load_zacks_table(\"MKTV_20240123.zip\")\n",
    "mktv['per_end_date'] = pd.to_datetime(mktv['per_end_date'])\n",
    "mktv = mktv[(mktv['ticker'].isin(universe_tickers)) & \n",
    "            (mktv['per_end_date'] >= PERIOD_START) & \n",
    "            (mktv['per_end_date'] <= PERIOD_END)].copy()\n",
    "\n",
    "fc = load_zacks_table(\"FC_20240123.zip\")\n",
    "fc['per_end_date'] = pd.to_datetime(fc['per_end_date'])\n",
    "fc['filing_date'] = pd.to_datetime(fc['filing_date'])\n",
    "fc = fc[(fc['ticker'].isin(universe_tickers)) & \n",
    "        (fc['per_end_date'] >= PERIOD_START) & \n",
    "        (fc['per_end_date'] <= PERIOD_END)].copy()\n",
    "\n",
    "fr = load_zacks_table(\"FR_20240123.zip\")\n",
    "fr['per_end_date'] = pd.to_datetime(fr['per_end_date'])\n",
    "fr = fr[(fr['ticker'].isin(universe_tickers)) & \n",
    "        (fr['per_end_date'] >= PERIOD_START) & \n",
    "        (fr['per_end_date'] <= PERIOD_END)].copy()\n",
    "\n",
    "shrs = load_zacks_table(\"SHRS_20240123.zip\")\n",
    "shrs['per_end_date'] = pd.to_datetime(shrs['per_end_date'])\n",
    "shrs = shrs[(shrs['ticker'].isin(universe_tickers)) & \n",
    "            (shrs['per_end_date'] >= PERIOD_START) & \n",
    "            (shrs['per_end_date'] <= PERIOD_END)].copy()\n",
    "\n",
    "print(f\"MKTV: {len(mktv):,} | FC: {len(fc):,} | FR: {len(fr):,} | SHRS: {len(shrs):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "load-prices",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading price data...\n",
      "  Chunk 5\n",
      "  Chunk 10\n",
      "\n",
      "Prices: 689,181 rows | 500 tickers | 2018-01-02 to 2023-06-30\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading price data...\")\n",
    "\n",
    "prices_list = []\n",
    "PRICES_ZIP = BASE_DIR / \"PRICES_20241105.zip\"\n",
    "PRICE_CSV = \"QUOTEMEDIA_PRICES_247f636d651d8ef83d8ca1e756cf5ee4.csv\"\n",
    "\n",
    "with zipfile.ZipFile(PRICES_ZIP) as z:\n",
    "    with z.open(PRICE_CSV) as f:\n",
    "        for i, chunk in enumerate(pd.read_csv(f, usecols=[\"ticker\", \"date\", \"adj_close\"],\n",
    "                                               parse_dates=[\"date\"], chunksize=5_000_000, \n",
    "                                               low_memory=False), 1):\n",
    "            chunk = chunk[(chunk[\"ticker\"].isin(universe_tickers)) &\n",
    "                         (chunk[\"date\"] >= PERIOD_START) &\n",
    "                         (chunk[\"date\"] <= PERIOD_END)]\n",
    "            prices_list.append(chunk)\n",
    "            if i % 5 == 0:\n",
    "                print(f\"  Chunk {i}\")\n",
    "\n",
    "prices = pd.concat(prices_list, ignore_index=True).sort_values(['ticker', 'date']).reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nPrices: {len(prices):,} rows | {prices['ticker'].nunique()} tickers | {prices['date'].min().date()} to {prices['date'].max().date()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prepare",
   "metadata": {},
   "source": [
    "## Prepare Fundamental Data\n",
    "\n",
    "Merge tables and calculate derived metrics:\n",
    "- **Debt**: Net long-term debt (or total if net unavailable) + current portion\n",
    "- **EPS**: Diluted EPS (or basic if unavailable), negative values set to 0.001\n",
    "- **Return**: Calculated from ROI and Investment (Market Cap + Debt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "prepare-fundamentals",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared fundamentals: 9,436 records | 500 tickers\n",
      "Filing dates: 2018-03-08 to 2023-12-29\n"
     ]
    }
   ],
   "source": [
    "fc_data = fc[['ticker', 'per_end_date', 'filing_date', 'per_type',\n",
    "              'tot_lterm_debt', 'net_lterm_debt', 'curr_portion_debt',\n",
    "              'eps_diluted_net', 'eps_basic_net']].copy()\n",
    "\n",
    "mktv_data = mktv[['ticker', 'per_end_date', 'mkt_val']].copy()\n",
    "shrs_data = shrs[['ticker', 'per_end_date', 'shares_out']].copy()\n",
    "fr_data = fr[['ticker', 'per_end_date', 'ret_invst']].copy()\n",
    "\n",
    "fundamentals = (fc_data\n",
    "    .merge(mktv_data, on=['ticker', 'per_end_date'], how='left')\n",
    "    .merge(shrs_data, on=['ticker', 'per_end_date'], how='left')\n",
    "    .merge(fr_data, on=['ticker', 'per_end_date'], how='left')\n",
    ")\n",
    "\n",
    "fundamentals = fundamentals[fundamentals['filing_date'].notna()].copy()\n",
    "fundamentals = fundamentals.sort_values(['ticker', 'per_end_date', 'per_type'])\n",
    "fundamentals = fundamentals.drop_duplicates(['ticker', 'per_end_date'], keep='first')\n",
    "\n",
    "fundamentals['debt'] = fundamentals['net_lterm_debt'].fillna(\n",
    "    fundamentals['tot_lterm_debt'].fillna(0) + fundamentals['curr_portion_debt'].fillna(0)\n",
    ")\n",
    "\n",
    "fundamentals['eps'] = fundamentals['eps_diluted_net'].fillna(fundamentals['eps_basic_net'])\n",
    "fundamentals.loc[fundamentals['eps'] < 0, 'eps'] = 0.001\n",
    "\n",
    "fundamentals['investment_at_report'] = fundamentals['mkt_val'] + fundamentals['debt']\n",
    "fundamentals['return'] = fundamentals['ret_invst'] * fundamentals['investment_at_report']\n",
    "\n",
    "print(f\"Prepared fundamentals: {len(fundamentals):,} records | {fundamentals['ticker'].nunique()} tickers\")\n",
    "print(f\"Filing dates: {fundamentals['filing_date'].min().date()} to {fundamentals['filing_date'].max().date()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daily-series",
   "metadata": {},
   "source": [
    "## Create Daily Time Series\n",
    "\n",
    "Fundamentals become known the day after `filing_date` and forward-fill until the next filing. Each ticker gets daily records with prices and forward-filled fundamental data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "build-daily",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daily records: 626,571 | 2018-03-09 to 2023-06-30\n"
     ]
    }
   ],
   "source": [
    "all_dates = pd.Series(sorted(prices['date'].unique()))\n",
    "daily_data_list = []\n",
    "\n",
    "for ticker in universe_tickers:\n",
    "    ticker_fundamentals = fundamentals[fundamentals['ticker'] == ticker].copy()\n",
    "    ticker_prices = prices[prices['ticker'] == ticker].copy()\n",
    "    \n",
    "    if len(ticker_fundamentals) == 0 or len(ticker_prices) == 0:\n",
    "        continue\n",
    "    \n",
    "    ticker_fundamentals['known_date'] = ticker_fundamentals['filing_date'] + pd.Timedelta(days=1)\n",
    "    \n",
    "    ticker_daily = ticker_prices[['date', 'adj_close']].copy()\n",
    "    ticker_daily['ticker'] = ticker\n",
    "    \n",
    "    ticker_daily = pd.merge_asof(\n",
    "        ticker_daily.sort_values('date'),\n",
    "        ticker_fundamentals[['known_date', 'per_end_date', 'mkt_val', 'debt',\n",
    "                             'eps', 'shares_out', 'return', 'ret_invst']].sort_values('known_date'),\n",
    "        left_on='date',\n",
    "        right_on='known_date',\n",
    "        direction='backward'\n",
    "    )\n",
    "    \n",
    "    daily_data_list.append(ticker_daily)\n",
    "\n",
    "daily_data = pd.concat(daily_data_list, ignore_index=True)\n",
    "daily_data = daily_data.dropna(subset=['per_end_date'])\n",
    "\n",
    "print(f\"Daily records: {len(daily_data):,} | {daily_data['date'].min().date()} to {daily_data['date'].max().date()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ratios",
   "metadata": {},
   "source": [
    "## Calculate Daily Ratios\n",
    "\n",
    "**Methodology:**\n",
    "- Get the stock price at each `per_end_date` (report date)\n",
    "- Scale market cap daily: `Market Cap(t) = Market Cap(report) × Price(t) / Price(report)`\n",
    "- Update ratios:\n",
    "  - **Debt/Market Cap** = Debt / Market Cap(t)\n",
    "  - **ROI** = Return / [Market Cap(t) + Debt]\n",
    "  - **P/E** = Market Cap(t) / (Shares × EPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "calc-ratios",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Ratios calculated\n",
      "\n",
      "  ticker       date  debt_to_mktcap       roi    pe_ratio\n",
      "0    DOV 2018-04-30       -0.024410  1.552509  110.355693\n",
      "1    DOV 2018-05-01       -0.024635  1.567239  109.343802\n",
      "2    DOV 2018-05-02       -0.025206  1.604492  106.867644\n",
      "3    DOV 2018-05-03       -0.025277  1.609089  106.570028\n",
      "4    DOV 2018-05-04       -0.024727  1.573210  108.939045\n",
      "5    DOV 2018-05-07       -0.024446  1.554916  110.189029\n",
      "6    DOV 2018-05-08       -0.024139  1.534859  111.593772\n",
      "7    DOV 2018-05-09       -0.023660  1.503676  113.852071\n",
      "8    DOV 2018-05-10       -0.023725  1.507895  113.541121\n",
      "9    DOV 2018-05-11       -0.023586  1.498884  114.207442\n",
      "\n",
      "NaN counts: Debt/MktCap=1,754 | ROI=3,274 | PE=4,859\n"
     ]
    }
   ],
   "source": [
    "report_prices_list = []\n",
    "\n",
    "for ticker in daily_data['ticker'].unique():\n",
    "    ticker_prices = prices[prices['ticker'] == ticker].sort_values('date')\n",
    "    ticker_reports = daily_data[daily_data['ticker'] == ticker][['per_end_date']].drop_duplicates()\n",
    "    \n",
    "    for per_end in ticker_reports['per_end_date'].unique():\n",
    "        prior_prices = ticker_prices[ticker_prices['date'] <= per_end]\n",
    "        if len(prior_prices) > 0:\n",
    "            price_at_report = prior_prices.iloc[-1]['adj_close']\n",
    "            report_prices_list.append({'ticker': ticker, 'per_end_date': per_end, \n",
    "                                      'price_at_report': price_at_report})\n",
    "\n",
    "report_prices_df = pd.DataFrame(report_prices_list)\n",
    "daily_data = daily_data.merge(report_prices_df, on=['ticker', 'per_end_date'], how='left')\n",
    "\n",
    "daily_data['mkt_val_current'] = daily_data['mkt_val'] * (daily_data['adj_close'] / daily_data['price_at_report'])\n",
    "\n",
    "daily_data['debt_to_mktcap'] = daily_data['debt'] / daily_data['mkt_val_current']\n",
    "\n",
    "daily_data['investment_current'] = daily_data['mkt_val_current'] + daily_data['debt']\n",
    "daily_data['roi'] = daily_data['return'] / daily_data['investment_current']\n",
    "\n",
    "daily_data['pe_ratio'] = daily_data['mkt_val_current'] / (daily_data['shares_out'] * daily_data['eps'])\n",
    "\n",
    "for col in ['debt_to_mktcap', 'roi', 'pe_ratio']:\n",
    "    daily_data[col] = daily_data[col].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "print(\"✓ Ratios calculated\\n\")\n",
    "print(daily_data[['ticker', 'date', 'debt_to_mktcap', 'roi', 'pe_ratio']].head(10))\n",
    "print(f\"\\nNaN counts: Debt/MktCap={daily_data['debt_to_mktcap'].isna().sum():,} | \"\n",
    "      f\"ROI={daily_data['roi'].isna().sum():,} | PE={daily_data['pe_ratio'].isna().sum():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "stats",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio Statistics:\n",
      "\n",
      "       debt_to_mktcap            roi      pe_ratio\n",
      "count   624817.000000  623297.000000  6.217120e+05\n",
      "mean         0.066763       1.991143  1.140419e+04\n",
      "std          0.285017     159.817336  8.232533e+04\n",
      "min         -2.629277  -11777.560214  5.026296e-01\n",
      "25%         -0.009670      -0.246668  3.483581e+01\n",
      "50%          0.003314       1.688442  9.044871e+01\n",
      "75%          0.071976       4.583504  3.759990e+03\n",
      "max          7.951443   20521.023094  4.364350e+06\n",
      "\n",
      "Total observations: 626,571\n",
      "Tickers: 500\n",
      "Date range: 2018-03-09 to 2023-06-30\n",
      "Average days per ticker: 1253\n"
     ]
    }
   ],
   "source": [
    "print(\"Ratio Statistics:\\n\")\n",
    "print(daily_data[['debt_to_mktcap', 'roi', 'pe_ratio']].describe())\n",
    "\n",
    "print(f\"\\nTotal observations: {len(daily_data):,}\")\n",
    "print(f\"Tickers: {daily_data['ticker'].nunique()}\")\n",
    "print(f\"Date range: {daily_data['date'].min().date()} to {daily_data['date'].max().date()}\")\n",
    "print(f\"Average days per ticker: {len(daily_data) / daily_data['ticker'].nunique():.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00210927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving daily ratios to Excel...\n",
      "✓ Saved to: C:\\Users\\15126\\Desktop\\Chicago\\Winter\\qts\\hw\\quantile_strats\\daily_ratios.xlsx\n",
      "Rows: 626,571 | Columns: 11\n"
     ]
    }
   ],
   "source": [
    "# Save daily ratios to Excel\n",
    "print(\"Saving daily ratios to Excel...\")\n",
    "\n",
    "output_path = r'C:\\Users\\15126\\Desktop\\Chicago\\Winter\\qts\\hw\\quantile_strats\\daily_ratios.xlsx'\n",
    "\n",
    "# Select key columns for export\n",
    "columns_to_save = [\n",
    "    'ticker', 'date', 'adj_close', 'per_end_date',\n",
    "    'mkt_val', 'debt', 'eps', 'shares_out',\n",
    "    'debt_to_mktcap', 'roi', 'pe_ratio'\n",
    "]\n",
    "\n",
    "daily_data[columns_to_save].to_excel(output_path, index=False, engine='openpyxl')\n",
    "\n",
    "print(f\"✓ Saved to: {output_path}\")\n",
    "print(f\"Rows: {len(daily_data):,} | Columns: {len(columns_to_save)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
